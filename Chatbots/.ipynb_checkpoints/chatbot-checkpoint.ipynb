{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Dd2p9uwChm"
   },
   "source": [
    "Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HfocuiJoOfYj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random\n",
    "#import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yw9LkWdEv89Y"
   },
   "source": [
    "Importing the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1nz0E_pO50z",
    "outputId": "16078de5-2685-4d0d-bf58-13cc467426d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\MR. DHRUV\n",
      "[nltk_data]     VAIDH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\MR. DHRUV\n",
      "[nltk_data]     VAIDH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f = open(\"chatbot.txt\",'r',errors = 'ignore')\n",
    "raw_doc = f.read()\n",
    "\n",
    "#Pre Processing and Text Case Handling\n",
    "\n",
    "raw_doc = raw_doc.lower()#converts everything inside the file into lower case\n",
    "nltk.download('punkt')# we are going to use punkt tokenizer as it is pre trained for making chatbots.\n",
    "nltk.download('wordnet')# we are going to use the wordnet dictionary\n",
    "sent_tokens  = nltk.sent_tokenize(raw_doc)# converts doc into a list of sentences(each element in this list is a sentance)\n",
    "word_tokens  = nltk.word_tokenize(raw_doc)# converts doc into a list of words(each element in this list is a word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJqVpsh9wUeK"
   },
   "source": [
    "Example of Sentence Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GV36AIRhO6j1",
    "outputId": "cdbb38c1-1889-4645-c482-893ac65e6e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data,[1][2] and apply knowledge and actionable insights from data across a broad range of application domains.',\n",
       " 'data science is related to data mining, machine learning and big data.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ajkh4MlnwbN2"
   },
   "source": [
    "Example of Word Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-FCHWp2Rg5j",
    "outputId": "bd23b65c-b05b-4cf5-a44e-7f674f031613"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'science']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViZQPcFI1-iQ"
   },
   "source": [
    "Text Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BBprQ1aPU0n0"
   },
   "outputs": [],
   "source": [
    "# WordNet dictinary i already included in the nltk library\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "# WordNet is a semantically-oriented dictionary of English in NLTK\n",
    "def LemTokens(tokens):\n",
    "  return [lemmer.lemmatize(token) for token in tokens]\n",
    "# We are removing the punctuations from the text here\n",
    "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7671ECP5ULh"
   },
   "source": [
    "Greeting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IpBwr63Z2Xs4"
   },
   "outputs": [],
   "source": [
    "# this list contains all the possible greetings a user can make to the bot\n",
    "GREET_INPUTS = [\"hello\" , \"hi\" , \"greetings\" , \"sup\" , \"what's up\" , \"hey\"]\n",
    "# this list contains some responses the bot can make after the user's greetings \n",
    "GREET_OUTPUTS = [\"hi\" , \"hey\" , \"*nods*\" , \"hi there\" , \"hello\" , \"I'm glad! that you're talking to me\"]\n",
    "def greet(sentence):\n",
    "# When the user enters a sentence, it is being split into words and each is being checked inside the GREET_INPUTS list\n",
    "  for word in sentence.split():\n",
    "    if word.lower() in GREET_INPUTS:\n",
    "      return random.choice(GREET_OUTPUTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSzA3-rxcGpW"
   },
   "source": [
    "Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VQasc5kF5ZzE"
   },
   "outputs": [],
   "source": [
    "# We use Text Frequency and Inverse Document Frequency Vectorizer is used to find how many times a term is present in the corpus(Text Frequency)\n",
    "# Inverse Document Frequency is used to find out how rare a term is in the corpus and it is converted into a vector form which is machine readable\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# After we obtain the the Bag of Words and their correcponding vectors, we use the cosine functionality to give a normalized output\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-Yrnv_Cw8rLw"
   },
   "outputs": [],
   "source": [
    "# We are converting the bag of words into vector form so that it is machine readable\n",
    "# These vectors are converted into matrices which will be used to generate responses\n",
    "def response(user_response):\n",
    "  robo1_response = ''\n",
    "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words = \"english\")\n",
    "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "  vals = cosine_similarity(tfidf[-1] , tfidf)\n",
    "  idx = vals.argsort()[0][-2]\n",
    "  flat = vals.flatten()\n",
    "  flat.sort()\n",
    "  req_tfidf = flat[-2]\n",
    "  if (req_tfidf == 0):\n",
    "    robo1_response = \"I'm Sorry\" + \"/n\" + \"I don't understand you\"\n",
    "  else:\n",
    "    robo1_response = robo1_response + sent_tokens[idx]\n",
    "    return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0El_GEE9j8T"
   },
   "source": [
    "Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8w3i6ihMcCQ_",
    "outputId": "51ee5a87-5e22-465a-f122-18e1ce8742b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOT: Welcome to Eduauraa.\n",
      "Let's have a conversation.\n",
      "If you want to exit, just type 'Bye'.\n",
      "hey\n",
      "BOT: I'm glad! that you're talking to me\n",
      "BOT: \n",
      "Press 1 to know about the Courses being offered\n",
      "Press 2 to Book a Free Trial\n",
      "Press 3 for Creating a New Account\n",
      "Press 4 for Customer Care Details \n",
      " Type 'bye' to exit this conversation\n",
      "1\n",
      "BOT: Courses\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "print(\"BOT: \",end=\"\")\n",
    "print(\"Welcome to Eduauraa.\"+\"\\n\"+\"Let's have a conversation.\"+\"\\n\"+\"If you want to exit, just type 'Bye'.\")\n",
    "#with open('contact', 'wb') as contacts_file\n",
    "while (flag==True):\n",
    "  class contact_details:\n",
    "    def __init__(self):\n",
    "        self.first_name = \"\"\n",
    "        self.last_name = \"\"\n",
    "        self.email_id =\"\"\n",
    "        self.phone_number = \"\"\n",
    "        self.state = \"\"\n",
    "        self.city = \"\"\n",
    "        self.board = \"\"\n",
    "        self.grade = \"\"\n",
    "        self.account_type = \"\"\n",
    "    def enter_data(self,a):\n",
    "      self.first_name = input(\"First Name: \") \n",
    "      self.last_name = input(\"Last Name: \") \n",
    "      self.email_id = input(\"Email ID: \") \n",
    "      self.phone_number = input(\"Phone Number: \")\n",
    "      self.state = input(\"State: \") \n",
    "      self.city = input(\"City: \") \n",
    "      self.board = input(\"Board: \")\n",
    "      self.grade = input(\"Grade: \") \n",
    "      if (int(a)==2):\n",
    "        self.account_type = 'trial'\n",
    "      elif (int(a)==3):\n",
    "        self.account_type = 'member'\n",
    "\n",
    "        \n",
    "  user_response = input()\n",
    "  user_response = user_response.lower()\n",
    "  if (user_response != 'bye'):\n",
    "    if (user_response =='thanks' or user_response =='thank you'):\n",
    "      flag = False\n",
    "      print(\"BOT: You are welcome.......\")\n",
    "    else:\n",
    "      if (greet(user_response) != None):\n",
    "        print(\"BOT: \"+greet(user_response))\n",
    "        print(\"BOT: \",end=\"\\n\")\n",
    "        print(\"Press 1 to know about the Courses being offered\")\n",
    "        print(\"Press 2 to Book a Free Trial\")\n",
    "        print(\"Press 3 for Creating a New Account\")\n",
    "        print(\"Press 4 for Customer Care Details \")\n",
    "        print(\" Type 'bye' to exit this conversation\")\n",
    "      if user_response == '1':\n",
    "          print(\"BOT: \",end=\"\")\n",
    "          print(\"Courses\")\n",
    "      elif user_response == '2':\n",
    "          print(\"BOT: \",end=\"\")\n",
    "          print(\"Enter Contact Details for Booking a Free Trial\")\n",
    "          user = contact_details()\n",
    "          user.enter_data(user_response)\n",
    "          #pickle.dump(user,contacts)\n",
    "\n",
    "      elif user_response == '3':\n",
    "          print(\"BOT: \",end=\"\")\n",
    "          print(\"Enter Contact Details for Creating a New Account\")\n",
    "          user = contact_details()\n",
    "          user.enter_data(user_response)\n",
    "          #pickle.dump(user,contacts)\n",
    "\n",
    "      elif user_response == '4':\n",
    "          print(\"BOT: \",end=\"\")\n",
    "          print(\"Customer Care Contact Details\")\n",
    "\n",
    "  else:\n",
    "    flag = False\n",
    "    print(\"BOT: Goodbye!!......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXJbZ8RE-whN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
